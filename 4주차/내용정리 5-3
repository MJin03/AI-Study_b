[05-3 트리의 앙상블]

* 정형 데이터 (structured data)
 : CSV 파일에 가지런히 정리되어있는 형태의 데이터.
   프로그래머가 다루는 대부분의 데이터는 정형 데이터이다.
   - (4장) 생선의 길이, 높이, 무게 등.
   - (5장) 와인 데이터
 
* 비정형 데이터 (unstructured data)
 : 정형 데이터의 반대.
   데이터베이스나 엑셀로 표현하기 어려운 데이터.
   - 책의 글과 같은 텍스트 데이터.
   - 디지털카메라로 찍은 사진
   - 핸드폰으로 듣는 디지털 음악 등.
 
* 앙상블 학습 (ensemble learning)
 : 정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘. (대부분 결정트리를 기반으로 만들어졌다.)
   - 정형 데이터의 끝판왕.

* 랜덤 포레스트 (Random Forest)
 : 결정트리의 앙상블.
   결정 트리를 랜덤하게 만들어 결정 트리(나무)의 숲을 만든 후 각 결정 트리의 예측을 사용해 최종 예측을 만든다.
    - 앙상블 학습의 대표 중 하나로 안정적인 성능으로 널리 사용되고 있다. (앙상블 학습 시 가장 먼저 사용해보길 권장.)
    - DecisionTreeClassifier가 제공하는 중요한 매개 변수를 모두 제공.
      >> criterion, maz_depth, maz_features, min_samples_split, min_impurity_decrease, min_samples_leaf 등 
    - 결정 트리의 가장 큰 장점 중 하나인 특성 중요도를 계산한다.
      >> 랜덤 포레스트 특성 중요도 : 각 결정 트리의 특성 중요도를 취합한 것.

   - 사이킷런의 랜덤 포레스트는 기본적으로 100개의 결정 트리를 위의 방식으로 훈련한다.
   -- (분류) : 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측으로 삼는다.
   -- (회귀) : 각 트리의 예측을 평균한다.

     *********************************************
     * 분류 : 샘플의 몇 개의 클래스 중 하나로 분류.  *
     * 회귀 : 임의의 어떤 숫자를 예측.              *
     *********************************************

   - 랜덤하게 선택한 샘플과 특성을 사용하기 때문에 훈련 세트에 과대적합되는 것을 막아주고 검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있다.
   - 최적의 분할을 찾는 데 시간을 많이 소모한다. 특히, 고려해야할 특성의 개수가 많을 때 시간이 더 많이 소모된다. -> 무작위로 나눈다면 훨씬 빨리 트리 구성 가능. (엑스트라 트리 사용.)
  
  > 부트스트랩 샘플 (bootstrap sample)
    : 랜덤 포레스트가 각 트리를 훈련하기 위해 랜덤하게 만드는 데이터로 데이터 세트에서 중복을 허용하여 데이터를 샘플링하는 방식을 의미한다.
      부트스트랩 샘플 == 부트스트랩 방식으로 샘플링하여 분류한 데이터.
     - 우리가 입력한 훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터를 만든다. (이때 한 샘플이 중복되어 추출될 수 있다.)
     - 기본적으로 부트스트랩 샘플은 훈련 세트의 크기와 같게 만든다.
    
       Ex). 1,000개의 가방에서 100개씩 샘플을 뽑는다면 먼저 1개를 뽑고, 뽑았던 1개를 다시 가방에 넣는다. 이 방식을 게속 반복하는데, 이런 식으로 100개를 가방에서 뽑는다면 중복된 샘플을 뽑을 수 있다.
          >> 1,000개 가방에서 중복하여 1,000개의 샘플을 뽑음 -> 부트스트랩 샘플의 크기 == 훈련 세트의 크기.

     - 노드를 분류할 때 전체 특성 중에서 일부 특성을 무작위로 고른 다음 이 중에서 최선의 분할을 찾는다. 
     -- 분류 모델 RandomForestClassifier: 전체 특성 개수의 제곱근만큼의 특성을 선택한다.  ->  특성:4개 --> 노드마다 2개를 랜덤하게 선택하여 사용.
     -- 회귀 모델 RandomForestRegressor: 전체 특성 사용.
  
  > OOB (out of bag)
    : 부트스트랩 샘프에 포함되지 않고 남는 샘플.
      - 검증세트의 역할을 진행.


<RandomForestClassifier 클래스를 화이트 와인 분류하는 문제에 적용>
** 와인 데이터셋을 판다스로 불러오고 훈련 세트, 테스트 세트 분리했다 가정**

- cross_validate() 함수 사용. 
 : 교차 검증 수행.
   > RandomForestClassifier는 기본적으로 100개의 결정 트리를 사용하므로 n_jobs = -1 지정. 
     : 최대한 병렬교차 검증을 수행.

   > return_train_score = True 지정.
     : 검증 점수뿐만 아니라 훈련 세트에 대한 점수도 같이 반환.
     - 기본값: False
     
- 랜덤 포레스트 모델을 훈련 세트에 훈련한 후 특성 중요도를 출력. 
  -> 결과: 당도 중요도 ↓, 알코올 도수 ↑, pH ↑
    >> 랜덤 포레스트가 일부를 랜덤하게 선택하여 결정 트리를 룬련하기 때문. (하나의 특성에 과도하게 집중X, 좀 더 많은 특성이 룬련에 기여할 기회O --> 과대 적합 ↓, 일반화 성능 ↑)

- OOB 점수 출력. 
  > RandomForestClassifier 클래스의 oob_score = True 지정. (oob_score의 기본 값: False)
    : 랜덤 포레스트는 각 결정 트리의 OOB 점수를 평균하여 출력. 
    -> 결과: 교차 검증과 매우 비슷한 값 출력. --> 교차 검증을 대신하여 결과적으로 훈련 세트에 더 많은 샘플을 사용할 수 있음.

  
* 엑스트라 트리 (Extra Tred) 
 : 랜덤 포레스트와 동일하게 결정 트리가 제공하는 대부분의 매개변수를 지원하고, 전체 특성 중에 일부 특성을 랜덤하게 선택하여 노드를 분할하는데 사용한다. 하지만 엑스트라 트리에서는 부트스트랩 샘플을 사용하지 않는다.
   - 사이킷런에서 제공하는 엑스트라 트리
    -- (분류) : ExtraTreesClassifier
    -- (회귀) : ExtraTreesRegressor  
   - 기본적으로 100개의 결정 트리를 훈련.
   - 결정트리를 만들 때 전체 훈련세트를 사용.
   - 노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할.
   - 매개변수 = 'random' 형태. 
   - 많은 트리를 앙상블하기 때문에 과대적합을 막고 검증 세트의 점수를 높이는 효과가 있다.
   - 특성이 많지 않은 경우 랜덤 포레스트 모델과의 차이가 크지 X
   - 보통 엑스트라 트리가 무작위성이 더 크기 때문에 앤덤 포레스트보다 더 많은 결정 트리를 훈련해야한다. -> 노드는 랜덤하게 분할하기 때문에 게산 속도가 빠르다. (엑스트라 트리의 장점)
   - 특성 중요도 제공. 
     Ex). 와인 데이터셋. --> 결과: 결정트리보다 당도에 대한 의존성이 작다.

* 그레이디언트 부스팅 (fradient boosting) 
 : 깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블하는 방법.
 - 깊이가 얕다 --> 과대적함에 강하고 일반적으로 높은 일반화 성능 기대 가능.
 - 과대적합을 잘 억제하면서 그레이디언트 부스팅보다 조금 더 높은 성능을 제공.
 
 - 사이킷런에서 제공하는 그레이디언트 부스팅 클래스
   -- (분류) : GradientBoostingClassifier
   -- (회귀) : GradientBoostingRegressor
 
 - (4장) 경사하강법을 사용하여 트리를 앙상블에 추가. 
  -- (분류) : 로지스틱 손실 함수.
  -- (회귀) : 평균 제곱 오차 함수.

 - 결정 트리를 계속 추가하면서 가장 낮은 곳을 찾아 이동한다. (학습률 매개변수(learning_rate)로 속도를 조절하여 천천히 조금씩 이동) --> 깊이가 걑은 트리를 사용하는 이유.
 - 결정 트리의 개수를 늘려도 과대적합에 매우 강하다. (학습률을 증가시키고(기본 값: 0.1) 트리의 개수를 늘리면 조금 더 성능 향상 가능) 

 - 득성 중요도 제공
   --> 위의 예제를 그레이디언트 부스팅으로 학습했을 때 결과: 랜덤 포레스트보다 일부 특성(당도)에 더 집중한다.

 - subsample: 훈련 세트의 비율을 정하는 매개변수.
   -- 기본 값: 1.0 (전체 훈련 세트 사용. 만약 1보다 작으면 훈련세트의 일부를 사용.) 
   -- 확률적 경사 하강법 (경사하강법 단계마다 일부 샘플을 랜덤하게 선택하여 진행.), 미니 경사 하강법과 비슷.

 - 일반적으로 랜덤 포레스트보다 조금 더 높은 성능을 얻을 수 있지만 순서대로 트리를 추가하기 때문에 훈련 속도가 느리다. 
   --> GradientBoostingRegressor에는 n_jobs 매개변수가 없다.

* 히스토그램 기반 그레이디언트 부스팅 (Histogram-based Gradient Boosting)
 : 그레이디언트 부스팅의 속도와 성능을 더욱 개선한 것.
   정형 데이터를 다루는 머신러닝 알고리즘 중에 가장 인기가 높은 알고리즘이다. 
   - 사이킷런의 히스토그램 기반 그레이디언트 부스팅 클래스
     -- (분류) : HistGradientBoostingClassifier
     -- (회귀) : HistGradientBoostingRegressor
     *** 히스토그램 기반 그레이디언트 부스팅은 아직 테스트 과정에 있으므로 클래스를 사용하려면 sklearn.experimental 패키지 아래있는 enable_hist_gradient_boosting 모듈을 임포트 해야한다.
   
   - 입력 특성: 256개의 구간으로 나눔. --> 노드를 분할할 때 최적의 분할은 매우 빠르게 찾을 수 있다.
   - 256개의 구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용한다. --> 입력에 누락된 특성이 있더라도 따로 전처리 할 필요 X.
   - HistGradientBoostingClassifier는 기본 매개변수에서 안정적인 성능을 얻을 수 있다.
     -- 트리 개수 지정: max_iter (부스팅 반복 횟수를 지정.)
   
   - 특성 중요도
     -- permutation_importance() 함수 사용.
       : 특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변화하는지를 관찰하여 어떤 특성이 중요한지를 계산한다.
        >> 훈련세트, 테스트세트, 사이킷런에서 제공하는 추정기 모델에 모두 사용 가능.

     -- n_repeats 매개변수
       : 랜덤하게 섞을 횟수를 지정. 
        >> 기본 값: 5

     -- permutation_importance() 함수
       : 함수가 반환하는 객체는 반복하여 얻은 득성 중요도(importances), 평균(importances_mean), 표준편차(importances_std)를 담고 있다.
   
   --> 결과: 조금 더 당도에 집중. (그레이디언트 부스팅과 비슷.)

 - 히스토그램 기반 그레이디언트 부스팅 알고리즘을 구현한 라이브러리
   -- XGBoost
    --- cross_validate() 함수와 함께 사용 가능.
    --- tree_method = 'hist'로 지정하면 히스토그램 기반 그레이디언트 부스팅 사용 가능.

   -- LightGBM
    --- 책에 내용이 거의 없었는데 아직 찾아보지 못해써요 나중에 정리할 때 추가할게요!




